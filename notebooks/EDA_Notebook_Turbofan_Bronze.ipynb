{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e93238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import logging\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Show more columns\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "# Initiate Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories():\n",
    "\n",
    "    # Get Current Working Directory\n",
    "    CWD = Path().resolve()\n",
    "\n",
    "    # \n",
    "    # Priority: ENV → script → Jupyter fallback\n",
    "    env_root = os.getenv(\"APP_ROOT\")\n",
    "\n",
    "    if env_root:\n",
    "        APP_ROOT = Path(env_root).resolve()\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            # Script Path\n",
    "            APP_ROOT = Path(__file__).resolve().parents[1]\n",
    "        except NameError:\n",
    "            # Notebook execution path\n",
    "            APP_ROOT = CWD.parent\n",
    "\n",
    "    # Set Data Directory\n",
    "    DATA_DIR = APP_ROOT / \"data\"\n",
    "\n",
    "    # Log everything for debugging\n",
    "    logger.info(f\"CWD:             {CWD}\")\n",
    "    logger.info(f\"APP_ROOT:        {APP_ROOT}\")\n",
    "    logger.info(f\"DATA Directory:  {DATA_DIR}\")\n",
    "\n",
    "    return APP_ROOT, DATA_DIR\n",
    "\n",
    "#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### \n",
    "\n",
    "def load_data(DATA_PATH, DATA_FILE, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "        Load data file with\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(DATA_PATH/DATA_FILE, **kwargs)\n",
    "        logger.info(f\"Data File Loaded: {data.info()}\")\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba348b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "# Folder Locations\n",
    "APP_ROOT, DATA_DIR = get_directories()\n",
    "\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "RAW_DATA_SUBDIR = \"NASA_Turbofan_Jet_Engine_Dataset/CMaps\"\n",
    "\n",
    "RAW_DATA_PATH = RAW_DATA_DIR / RAW_DATA_SUBDIR\n",
    "\n",
    "BRONZE_DATA_DIR = DATA_DIR / \"bronze\"\n",
    "BRONZE_DATA_DIR = \"turbofan_train_bronze.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RAW_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd81fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 26 columns in the dataset. \n",
    "# The first five columns have a different format then the remaining 21. \n",
    "# We will addresss these columns first then add in the remainingg 21. \n",
    "\n",
    "# Creating column name list for the first five. \n",
    "COLUMN_NAMES = [\n",
    "    \"unit\",\n",
    "    \"cycle\",\n",
    "    \"op_setting_1\",\n",
    "    \"op_setting_2\",\n",
    "    \"op_setting_3\",\n",
    "]\n",
    "\n",
    "# As stated the remaining 21 columns share a similar name structure of sensor then a number\n",
    "# So we will generate them using a loop and appended them to our list:\n",
    "COLUMN_NAMES += [f\"s{i}\" for i in range(1, 22)]\n",
    "\n",
    "# Display Column Names to verify success.\n",
    "len(COLUMN_NAMES), COLUMN_NAMES[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Import Keywords/Additional parameters.\n",
    "data_import_parameters = {\"sep\":\"\\s+\", \"header\":None, \"names\": COLUMN_NAMES}\n",
    "\n",
    "# Anomaly window for when errors occur\n",
    "ANOMALY_HORIZON = 30\n",
    "\n",
    "FD_IDS = [\"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n",
    "\n",
    "# Creating a list \n",
    "all_training_dfs = []\n",
    "\n",
    "for fd_id in FD_IDS:\n",
    "\n",
    "    RAW_FD_PATH = RAW_DATA_PATH\n",
    "    RAW_DATA_FILE = f\"train_{fd_id}.txt\"\n",
    "\n",
    "    raw_df = load_data(RAW_FD_PATH, RAW_DATA_FILE, **data_import_parameters)\n",
    "\n",
    "    raw_df.columns = COLUMN_NAMES\n",
    "\n",
    "    max_cycle = raw_df.groupby(\"unit\")[\"cycle\"].max().rename(\"max_cycle\")\n",
    "\n",
    "    df = raw_df.merge(max_cycle, on=\"unit\", how=\"left\")\n",
    "\n",
    "    df[\"RUL\"] = df[\"max_cycle\"] - df[\"cycle\"]\n",
    "\n",
    "    df = df.drop(columns=[\"max_cycle\"])\n",
    "\n",
    "    df[\"anomaly_flag\"] = (df[\"RUL\"] <= ANOMALY_HORIZON).astype(int)\n",
    "\n",
    "    df[\"dataset_name\"] = \"TURBOFAN\"\n",
    "    df[\"fd_id\"] = fd_id\n",
    "\n",
    "    df = df.rename(columns={\n",
    "                \"unit\": \"machine_id\",\n",
    "                \"cycle\": \"time_index\",\n",
    "                }, inplace=True)\n",
    "    \n",
    "    df[\"event_time\"] = df[\"time_index\"]\n",
    "\n",
    "    core_columns = [\"dataset_name\", \"fd_id\", \"machine_id\", \"time_index\", \"event_time\"]\n",
    "    label_columns = [\"RUL\", \"anomaly_flag\"]\n",
    "\n",
    "    data_columns_list = list(df.columns)\n",
    "\n",
    "    for column in core_columns + label_columns: \n",
    "        if column in data_columns_list:\n",
    "            data_columns_list.remove(column)\n",
    "\n",
    "    ordered_columns = core_columns + label_columns + data_columns_list\n",
    "\n",
    "    df_ordered = df[ordered_columns].copy()\n",
    "\n",
    "    all_training_dfs.append(df_ordered)\n",
    "\n",
    "\n",
    "bronze_dfs = pd.concat(all_training_dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa991cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_dfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb998e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_dfs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f32875",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path_all = (BRONZE_DATA_DIR) / \"turbofan_all_train_bronze.csv\"\n",
    "\n",
    "bronze_dfs.to_csv(out_path_all, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
