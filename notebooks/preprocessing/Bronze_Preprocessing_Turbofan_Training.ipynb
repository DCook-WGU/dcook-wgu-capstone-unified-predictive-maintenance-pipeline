{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e93238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import logging\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "# Custom Utilities Module\n",
    "from utils.paths import get_paths\n",
    "from utils.file_io import load_data\n",
    "\n",
    "\n",
    "# Show more columns\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "\n",
    "# Initiate Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Path's Object\n",
    "paths = get_paths()\n",
    "\n",
    "logger.info(f\"Project Root Path Loaded: {paths.root}\")    \n",
    "    \n",
    "logger.info(f\"Project Data Path Loaded: {paths.data}\")\n",
    "logger.info(f\"Data Raw Path Loaded: {paths.data_raw}\")\n",
    "\n",
    "logger.info(f\"Data Bronze Path Loaded: {paths.data_bronze}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd81fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 26 columns in the dataset. \n",
    "# The first five columns have a different format then the remaining 21. \n",
    "# We will addresss these columns first then add in the remainingg 21. \n",
    "\n",
    "# Creating column name list for the first five. \n",
    "COLUMN_NAMES = [\n",
    "    \"unit\",\n",
    "    \"cycle\",\n",
    "    \"op_setting_1\",\n",
    "    \"op_setting_2\",\n",
    "    \"op_setting_3\",\n",
    "]\n",
    "\n",
    "# As stated the remaining 21 columns share a similar name structure of sensor then a number\n",
    "# So we will generate them using a loop and appended them to our list:\n",
    "COLUMN_NAMES += [f\"s{i}\" for i in range(1, 22)]\n",
    "\n",
    "# Display Column Names to verify success.\n",
    "len(COLUMN_NAMES), COLUMN_NAMES[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Import Keywords/Additional parameters.\n",
    "data_import_parameters = {\"sep\":\"\\s+\", \"header\":None, \"names\": COLUMN_NAMES}\n",
    "\n",
    "# Anomaly window for when errors occur\n",
    "ANOMALY_HORIZON = 30\n",
    "\n",
    "FD_IDS = [\"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n",
    "\n",
    "# Creating a list \n",
    "all_training_dfs = []\n",
    "\n",
    "for fd_id in FD_IDS:\n",
    "\n",
    "    RAW_DATA_FILE = f\"train_{fd_id}.txt\"\n",
    "\n",
    "    raw_df = load_data(path.data_raw / \"CMaps\", RAW_DATA_FILE, **data_import_parameters)\n",
    "\n",
    "    raw_df.columns = COLUMN_NAMES\n",
    "\n",
    "    max_cycle = raw_df.groupby(\"unit\")[\"cycle\"].max().rename(\"max_cycle\")\n",
    "    logger.info(f\"Max Cycle Dataframe Created: {raw_df.info()}\")\n",
    "\n",
    "    df = raw_df.merge(max_cycle, on=\"unit\", how=\"left\")\n",
    "    logger.info(f\"Raw Import Merged with Max Cycle: {df.info()}\")\n",
    "\n",
    "    df[\"RUL\"] = df[\"max_cycle\"] - df[\"cycle\"]\n",
    "\n",
    "    df = df.drop(columns=[\"max_cycle\"])\n",
    "    logger.info(f\"RUL Calculcated and Added: {df.info()}\")\n",
    "\n",
    "    df[\"anomaly_flag\"] = (df[\"RUL\"] <= ANOMALY_HORIZON).astype(int)\n",
    "    logger.info(f\"Anomaly Flag Added: {df.info()}\")\n",
    "\n",
    "    df[\"dataset_name\"] = \"TURBOFAN\"\n",
    "    logger.info(f\"Column Added: 'dataset_name': {df.info()}\")\n",
    "\n",
    "    df[\"fd_id\"] = fd_id\n",
    "    logger.info(f\"Column Added: 'fd_id': {df.info()}\")\n",
    "\n",
    "    df[\"event_time\"] = df[\"cycle\"]\n",
    "    logger.info(f\"Column Added: 'event_time': {df.info()}\")\n",
    "\n",
    "    df = df.rename(columns={\n",
    "                \"unit\": \"machine_id\",\n",
    "                \"cycle\": \"time_index\",\n",
    "                })\n",
    "    logger.info(f\"Columns Renamed: 'unit'>'machine_id' & 'cycle'>'time_index'  {df.info()}\")\n",
    "\n",
    "    core_columns = [\"dataset_name\", \"fd_id\", \"machine_id\", \"time_index\", \"event_time\"]\n",
    "    logger.info(f\"Core Columns Assigned and stored to list: {core_columns}\")\n",
    "\n",
    "    label_columns = [\"RUL\", \"anomaly_flag\"]\n",
    "    logger.info(f\"Label Columns Assigned and stored to list: {label_columns}\")\n",
    "\n",
    "    data_columns = list(df.columns)\n",
    "    logger.info(f\"Data Columns List created from existing columns in dataset: {data_columns}\")\n",
    "\n",
    "    for column in core_columns + label_columns: \n",
    "        if column in data_columns:\n",
    "            data_columns.remove(column)\n",
    "\n",
    "    logger.info(f\"Data Columns List purged of core and label columns: {data_columns}\")\n",
    "\n",
    "    ordered_columns = core_columns + label_columns + data_columns\n",
    "    logger.info(f\"Column Order Set: {ordered_columns}\")\n",
    "\n",
    "    df_ordered = df[ordered_columns].copy()\n",
    "    logger.info(f\"Orderd Dataframe created: {df_ordered.info()}\")\n",
    "\n",
    "    all_training_dfs.append(df_ordered)\n",
    "    logger.info(f\"Orderd Dataframe appended to list of dataframes: {all_training_dfs}\")\n",
    "\n",
    "\n",
    "bronze_dfs = pd.concat(all_training_dfs, axis=0, ignore_index=True)\n",
    "logger.info(f\"All Dataframe concatenated into a single dataframe, vertically {bronze_dfs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa991cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_dfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb998e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_dfs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f32875",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_dfs.to_csv(BRONZE_DATA_OUTPUT_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
